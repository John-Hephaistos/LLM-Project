{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the padding token to eos_token since GPT-2 does not have a default padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/juliuselemans/Downloads/IMDB Dataset.csv\")\n",
    "def remove_non_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "df['review'] = df['review'].apply(remove_non_alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of preprocessed reviews\n",
    "print(\"Sample preprocessed reviews:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate token length for each review\n",
    "def token_length(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Apply function to the 'prompt' column\n",
    "df_token_length['token_length'] = df['review'].apply(token_length)\n",
    "\n",
    "# Save the dataframe with the token lengths to a new CSV\n",
    "df_token_length.to_csv(\"token_length_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    # Directly asking for a one-word label (positive or negative) to improve clarity\n",
    "    prompt = f\" Please classify the sentiment of this review in one word: positive or negative. Review: '{text}'.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,  # Keeping it concise to focus on getting just the sentiment label\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        #temperature=0.3,  # Lower temperature to encourage more predictable outputs\n",
    "        #top_p=0.8,  # Narrow the choice of possible tokens to increase likelihood of desired output\n",
    "        #no_repeat_ngram_size=2\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sentiment analysis function with a sample review\n",
    "test_review = \"This movie was an excellent portrayal of historical events.\"\n",
    "result = get_sentiment(test_review) \n",
    "print(\"Sentiment Prediction:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis and display some responses\n",
    "df['model_response'] = df['prompt'].apply(get_sentiment)\n",
    "print(\"Sample model responses:\")\n",
    "print(df[['prompt', 'model_response']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to directly use model's numeric output as sentiment\n",
    "def interpret_response(response):\n",
    "    try:\n",
    "        sentiment_score = int(response)  # Convert response to integer\n",
    "        if sentiment_score == 1 or sentiment_score == -1:\n",
    "            return sentiment_score\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return 0  # Return 0 for any non-numeric or unexpected output\n",
    "\n",
    "# Apply the interpretation function to model responses\n",
    "df['predicted_sentiment'] = df['model_response'].apply(interpret_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary metrics calculation library\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming 'completion' contains 'positive' or 'negative', map these to numeric values\n",
    "df['actual_sentiment'] = df['completion'].replace({'positive': 1, 'negative': -1})\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(df['actual_sentiment'], df['predicted_sentiment'])\n",
    "precision = precision_score(df['actual_sentiment'], df['predicted_sentiment'], pos_label=1)\n",
    "recall = recall_score(df['actual_sentiment'], df['predicted_sentiment'], pos_label=1)\n",
    "f1 = f1_score(df['actual_sentiment'], df['predicted_sentiment'], pos_label=1)\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file with all model responses, prompts, and predicted sentiments\n",
    "df.to_csv('model_responses_and_sentiments.csv', index=False)\n",
    "print(\"Data has been written to 'model_responses_and_sentiments.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
